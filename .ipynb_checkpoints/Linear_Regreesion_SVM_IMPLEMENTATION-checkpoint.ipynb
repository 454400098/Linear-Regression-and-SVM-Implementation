{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetgenerator:\n",
    "    def __init__(self,size):\n",
    "        self.train_arr = []\n",
    "        self.size = size\n",
    "        self.sigma = math.sqrt(0.1)\n",
    "        self.Y = []\n",
    "        self.gene()\n",
    "        \n",
    "    def gene(self):\n",
    "        for _ in range(self.size):\n",
    "            tmparr = []\n",
    "            a = np.asarray([1])\n",
    "            tmpfirst10 = random.normal(0,self.sigma,10)\n",
    "            tmpfirst10 = np.concatenate((a, tmpfirst10), axis=None)\n",
    "            for var in tmpfirst10:\n",
    "                tmparr.append(var)\n",
    "            tmparr.append(tmparr[0]+tmparr[1]+np.random.normal(0,self.sigma))\n",
    "            tmparr.append(tmparr[2]+tmparr[3]+np.random.normal(0,self.sigma))\n",
    "            tmparr.append(tmparr[3]+tmparr[4]+np.random.normal(0,self.sigma))\n",
    "            tmparr.append(0.1*tmparr[6]+np.random.normal(0,self.sigma))\n",
    "            tmparr.append(2*tmparr[1]-10+np.random.normal(0,self.sigma))\n",
    "            # x16~x20\n",
    "            tmp16 = np.random.normal(0,self.sigma,5)\n",
    "            for var in tmp16:\n",
    "                tmparr.append(var)\n",
    "            self.train_arr.append(tmparr)\n",
    "            tmpy = self.computeY(tmparr)\n",
    "            self.Y.append(tmpy)\n",
    "            \n",
    "    def computeY(self,arr):\n",
    "        tmparr = arr\n",
    "        y = 0\n",
    "        i = 1\n",
    "        for i in range(1,11):\n",
    "            y+=(0.6)**i*tmparr[i-1]\n",
    "        y+=10+np.random.normal(0,self.sigma)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasetgenerator(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression:\n",
    "    def __init__(self,train_arr,Y,lamda):\n",
    "        self.arr_x = np.asarray(train_arr)\n",
    "        self.arr_y = np.asarray(Y)\n",
    "        self.lamda = lamda\n",
    "    def LSR(self):\n",
    "        w = np.dot(inv(np.dot(self.arr_x.T,self.arr_x)),np.dot(self.arr_x.T,self.arr_y))\n",
    "        return w\n",
    "    \n",
    "    def ridge(self):\n",
    "        w = np.dot(inv(np.dot(self.arr_x.T,self.arr_x)+self.lamda*np.identity(21)),np.dot(self.arr_x.T,self.arr_y))\n",
    "        return w\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def updatebias(self,w,x,y):\n",
    "        sum = 0\n",
    "        for i in range(len(y)):\n",
    "            sum+=y[i]-np.dot(w,x[i])\n",
    "    \n",
    "        sum= sum/(len(y))\n",
    "        w[0] = w[0]+sum\n",
    "    \n",
    "    def updateunbias(self,w,x,y,row,lamda):\n",
    "        wi = w[row]\n",
    "        #compute the big part\n",
    "        tmpx = self.prunex(x,row)\n",
    "        xt = tmpx.T\n",
    "        numer = np.dot(xt,y-np.dot(x,w))-lamda/2\n",
    "        deno = np.dot(xt,tmpx)\n",
    "        \n",
    "        ans1 = numer/deno\n",
    "        \n",
    "        numer2 =  np.dot(xt,y-np.dot(x,w))+lamda/2\n",
    "        ans2 = numer2/deno\n",
    "        \n",
    "        #first case\n",
    "        if -ans1 < wi:\n",
    "            w[row]=wi+ans1\n",
    "        elif wi < -ans2:\n",
    "            w[row]=wi+ans2\n",
    "        elif wi>=-ans2 and wi<=-ans1:\n",
    "            w[row] = 0\n",
    "\n",
    "    def Lasso(self,lamda):\n",
    "        #create routine to update bias\n",
    "        #routine to update unbias\n",
    "        w =  np.full((21),1.0)\n",
    "        for _ in range(100):\n",
    "            for i in range(21):\n",
    "                if i == 0:\n",
    "                    self.updatebias(w,self.arr_x,self.arr_y)\n",
    "                else:\n",
    "                    self.updateunbias(w,self.arr_x,self.arr_y,i,lamda)\n",
    "        return w\n",
    "    \n",
    "    \n",
    "        \n",
    "    def prunex(self,x,row):\n",
    "        tmpx = []\n",
    "        for i in range(len(x)):\n",
    "            tmpx.append(x[i][row])\n",
    "        tmpx = np.asarray(tmpx)\n",
    "        return tmpx\n",
    "                       \n",
    "#     preditct section\n",
    "    def predict(self,w):\n",
    "        err = 0\n",
    "        arr_y = np.dot(self.arr_x,w)\n",
    "        for i in range(len(self.arr_y)):\n",
    "            err+=abs(arr_y[i]-self.arr_y[i])**2\n",
    "        return err/2\n",
    "    \n",
    "    \n",
    "    def predict_true(self,x,y,w):\n",
    "        arr_x = np.asarray(x)\n",
    "        arr_y = np.asarray(y)\n",
    "        err = 0\n",
    "        arr_y_new = np.dot(arr_x,w)\n",
    "        for i in range(len(arr_y)):\n",
    "            err+=abs(arr_y_new[i]-arr_y[i])**2\n",
    "        return err/2\n",
    "    \n",
    "    \n",
    "    def ridge_predict(self,w):\n",
    "        err = 0\n",
    "        arr_y = np.dot(self.arr_x,w)\n",
    "        for i in range(len(self.arr_y)):\n",
    "            err+=abs(arr_y[i]-self.arr_y[i])**2\n",
    "        err+=np.dot(w,w)*0.5*self.lamda\n",
    "        return err/2\n",
    "    \n",
    "    def true_ridge_predict(self,x,y,w):\n",
    "        arr_x = np.asarray(x)\n",
    "        arr_y = np.asarray(y)\n",
    "        err = 0\n",
    "        arr_y_new = np.dot(arr_x,w)\n",
    "        for i in range(len(arr_y)):\n",
    "            err+=abs(arr_y_new[i]-arr_y[i])**2\n",
    "        err+=np.dot(w,w)*0.5*self.lamda\n",
    "        return err/2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Linear_Regression(d.train_arr,d.Y,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset of size 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasetgenerator(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Least Square regreesion model to fit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Linear_Regression(d.train_arr,d.Y,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = solver.LSR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out the the $\\underline{w}$ after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06107987e+01  3.49051515e-01  2.42289840e-01  1.41207074e-01\n",
      "  7.07439829e-02  3.13338976e-02  3.10735656e-02  3.55039573e-02\n",
      "  3.14759819e-02 -8.04133610e-03 -8.06906870e-04  9.19181565e-03\n",
      " -2.39783562e-02  7.27092811e-03 -2.12074934e-03  1.60000792e-03\n",
      " -1.50419527e-03 -1.18463064e-02 -1.98314360e-03  1.22167828e-03\n",
      " -9.55806159e-03]\n"
     ]
    }
   ],
   "source": [
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias value is $1.06107987e+01$ which is almost equal to 10, and for weights, they are lists above from $m[1]$ to $m[20]$\n",
    "For comparision, the bias is almost equal to the true bias, as for the weighs, in true weights, as $i$ increases the weight should be smaller, which is consistent with the weights that I get using least square regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most significant feature is $m[0] = 1.06107987e+01$ which is the bias, the least significant feature is $m[19]$, based on the result, $\\underline{w}$ could be prune by omiting $\\underline{x}$ from $x_{5}$ to $x_{20}$, since the weight vector is too small on these x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate large dataset to estimate the 'true' error \n",
    "By cutting the dataset in the following ways:\n",
    "80% of dataset will be used to train model, 20% dataset only used to predict and conclude true error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "d = datasetgenerator(8000)\n",
    "d2 = datasetgenerator(2000)\n",
    "for _ in range(100):\n",
    "    solver = Linear_Regression(d.train_arr,d.Y,0)\n",
    "    m = solver.LSR()\n",
    "    errtmp = solver.predict_true(d2.train_arr,d2.Y,m)\n",
    "    err.append(errtmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.28475479031786"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mean error shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasetgenerator(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set $\\lambda$ to be 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = Linear_Regression(d.train_arr,d.Y,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = solver.ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print weight when $\\lambda$ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.25500906e+00,  8.89002390e-01,  1.75714370e-01,  1.16726730e-01,\n",
       "        4.71673539e-02,  4.01228152e-03,  6.23867122e-02, -4.23862586e-02,\n",
       "       -5.20504483e-02,  2.73475515e-02, -1.50919247e-03,  6.93042122e-02,\n",
       "        4.79683535e-02,  2.37696104e-02,  5.92617537e-04, -3.26268197e-01,\n",
       "        2.17159121e-03,  1.73037415e-02,  4.86569446e-03, -3.39331849e-03,\n",
       "       -1.01752440e-02])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate the true error by evaluating that model on a large test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate large dataset to estimate the 'true' error\n",
    "By cutting the dataset in the following ways: 80% of dataset will be used to train model, 20% dataset only used to predict and conclude true error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = []\n",
    "d = datasetgenerator(8000)\n",
    "d2 = datasetgenerator(2000)\n",
    "for _ in range(100):\n",
    "    solver = Linear_Regression(d.train_arr,d.Y,0.5)\n",
    "    m = solver.ridge()\n",
    "    errtmp = solver.true_ridge_predict(d2.train_arr,d2.Y,m)\n",
    "    err.append(errtmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.9849253736518"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The mean error shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot estimated true error of the ridge regression model as a funciton of $\\lambda$ find optimal $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "err = []\n",
    "errlist = {}\n",
    "weightlist = {}\n",
    "while lamda < 5:\n",
    "    d = datasetgenerator(800)\n",
    "    d2 = datasetgenerator(200)\n",
    "    solver = Linear_Regression(d.train_arr,d.Y,lamda)\n",
    "    m = solver.ridge()\n",
    "    errtmp = solver.true_ridge_predict(d2.train_arr,d2.Y,m)\n",
    "    errlist[errtmp]=lamda\n",
    "    err.append(errtmp)\n",
    "    weightlist[lamda] = m\n",
    "    lamda+=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "err.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.401442995744265"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errlist[err[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the result, the optimal $\\lambda$ is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\underline{w}$ is shown below with $\\lambda = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.11955880e+01,  2.45341147e-01,  2.46216431e-01,  1.29131912e-01,\n",
       "        1.04523340e-01,  2.81842569e-02, -1.69076953e-02, -2.10010207e-02,\n",
       "       -1.35925217e-02,  4.35517931e-02, -1.74618239e-02, -2.63568820e-02,\n",
       "       -6.63512057e-03,  7.09882434e-03,  6.40762809e-03,  5.67764819e-02,\n",
       "       -1.10625013e-02,  5.79067194e-02,  2.09943501e-02,  6.02264156e-02,\n",
       "        4.00006343e-03])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightlist.get(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The bias is $1.11955880e+01$ which is close to 10 as true bias, Also we could conclude that the computed weights are close to true weight, also the computed weights are in consistent with the true weights by showing that the weight with $i$ = 1 is largest, and weight is smaller when $i$ increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The Most significant feature is 10 which is bias with the $\\underline{w} = 1.11955880e+01$, the least significant features is $X$ when $i=11$ to $i = 20$ That means we can prune all feature from i = 11 to i = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The optimal ridge regression is same as naive least squares model since  i get smallest error by set $\\lambda = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show that m = 1000, as lamda increases, features are effectively eliminated from the model until all weights are set to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate $\\lambda$ from 1 to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamda = 0\n",
    "err = []\n",
    "errlist = {}\n",
    "wlist = []\n",
    "lamda = 1\n",
    "d = datasetgenerator(1000)\n",
    "while lamda < 100:\n",
    "#     d2 = datasetgenerator(200)\n",
    "    solver = Linear_Regression(d.train_arr,d.Y,lamda)\n",
    "    m = solver.Lasso(lamda)\n",
    "    wlist.append(m)\n",
    "    lamda+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lamda = 0, weight is : ',wlist[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
